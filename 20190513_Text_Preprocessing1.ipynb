{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20190513_Text Preprocessing1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leeminhyeon/Text_Preprocessing/blob/master/20190513_Text_Preprocessing1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OspPnqSf1vMm",
        "colab_type": "text"
      },
      "source": [
        "# **20190513_Text Preprocessing**\n",
        "# 1. 영문 전처리 실습\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhFQopIt_4rV",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 실습용 영문기사 수집\n",
        "온라인 기사를 바로 수집하여 실습데이터로 사용\n",
        "\n",
        "https://www.forbes.com/sites/adrianbridgwater/2019/04/15/what-drove-the-ai-renaissance/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltfUNpmh10WK",
        "colab_type": "code",
        "outputId": "ab71447c-7094-47bf-d7ea-4cddd4eaec8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = 'https://www.forbes.com/sites/adrianbridgwater/2019/04/15/what-drove-the-ai-renaissance/?ss=ai-big-data#45dd5dd61f25'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text,'html.parser')\n",
        "\n",
        "eng_news = soup.select('p') #[class=\"speakable-paragraph\"]')\n",
        "eng_text = eng_news[3].get_text()\n",
        "\n",
        "eng_text\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"And yes, she does mean everybody's job from yours to mine and onward to the role of grain farmers in Egypt, pastry chefs in Paris and dog walkers in Oregon i.e. every job. We will now be able to help direct all workers’ actions and behavior with a new degree of intelligence that comes from predictive analytics, all stemming from the AI engines we will now increasingly depend upon.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzwAx7ZEEfq2",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 영문 토큰화(Tokenization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Pa9ubE-_7bM",
        "colab_type": "code",
        "outputId": "6a878b7e-9c39-4bb2-c076-1cdcb0fac0cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "#Word_tokenize() : 마침표와 구두점(온점(.), 컴마(,), 물음표(?), 세미콜론(;), 느낌표(!) 등\n",
        "import nltk\n",
        "nltk.download('punkt') #로컬에 다운받아줘야 오류가 안남\n",
        "from nltk.tokenize import word_tokenize\n",
        "token1 = word_tokenize(eng_text)\n",
        "print(token1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['And', 'yes', ',', 'she', 'does', 'mean', 'everybody', \"'s\", 'job', 'from', 'yours', 'to', 'mine', 'and', 'onward', 'to', 'the', 'role', 'of', 'grain', 'farmers', 'in', 'Egypt', ',', 'pastry', 'chefs', 'in', 'Paris', 'and', 'dog', 'walkers', 'in', 'Oregon', 'i.e', '.', 'every', 'job', '.', 'We', 'will', 'now', 'be', 'able', 'to', 'help', 'direct', 'all', 'workers', '’', 'actions', 'and', 'behavior', 'with', 'a', 'new', 'degree', 'of', 'intelligence', 'that', 'comes', 'from', 'predictive', 'analytics', ',', 'all', 'stemming', 'from', 'the', 'AI', 'engines', 'we', 'will', 'now', 'increasingly', 'depend', 'upon', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZnmyWV_BJpa",
        "colab_type": "code",
        "outputId": "43980670-8503-4b55-a5e9-deb54ca11179",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#WordPunctTokenizer() : 알파벳과 알파벳이 아닌문자를 구분하여 토큰화\n",
        "import nltk\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "token2 = WordPunctTokenizer().tokenize(eng_text)\n",
        "print(token2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['And', 'yes', ',', 'she', 'does', 'mean', 'everybody', \"'\", 's', 'job', 'from', 'yours', 'to', 'mine', 'and', 'onward', 'to', 'the', 'role', 'of', 'grain', 'farmers', 'in', 'Egypt', ',', 'pastry', 'chefs', 'in', 'Paris', 'and', 'dog', 'walkers', 'in', 'Oregon', 'i', '.', 'e', '.', 'every', 'job', '.', 'We', 'will', 'now', 'be', 'able', 'to', 'help', 'direct', 'all', 'workers', '’', 'actions', 'and', 'behavior', 'with', 'a', 'new', 'degree', 'of', 'intelligence', 'that', 'comes', 'from', 'predictive', 'analytics', ',', 'all', 'stemming', 'from', 'the', 'AI', 'engines', 'we', 'will', 'now', 'increasingly', 'depend', 'upon', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kMqA0HLB1BJ",
        "colab_type": "code",
        "outputId": "a7873a5d-d7cd-427e-d8c8-58d9bfeb8388",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "#TreebankWordTokenizer() : 정규표현식에 기반한 토큰화\n",
        "import nltk\n",
        "nltk.download('punkt') #punctuation 정보 다운로드\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "token3 = TreebankWordTokenizer().tokenize(eng_text)\n",
        "print(token3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['And', 'yes', ',', 'she', 'does', 'mean', 'everybody', \"'s\", 'job', 'from', 'yours', 'to', 'mine', 'and', 'onward', 'to', 'the', 'role', 'of', 'grain', 'farmers', 'in', 'Egypt', ',', 'pastry', 'chefs', 'in', 'Paris', 'and', 'dog', 'walkers', 'in', 'Oregon', 'i.e.', 'every', 'job.', 'We', 'will', 'now', 'be', 'able', 'to', 'help', 'direct', 'all', 'workers', '’', 'actions', 'and', 'behavior', 'with', 'a', 'new', 'degree', 'of', 'intelligence', 'that', 'comes', 'from', 'predictive', 'analytics', ',', 'all', 'stemming', 'from', 'the', 'AI', 'engines', 'we', 'will', 'now', 'increasingly', 'depend', 'upon', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZjacOonEomq",
        "colab_type": "text"
      },
      "source": [
        "## 1.3 영문 품사 부착(PoS Tagging)\n",
        "분리한 토큰마다 품사를 부착한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaXAVV95FJtA",
        "colab_type": "code",
        "outputId": "cfcda414-4873-47e7-e2b7-1575acd1e32b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAO_VrxwFPS0",
        "colab_type": "code",
        "outputId": "59aa1041-59a0-44ca-d29d-cda9b39664f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "taggedToken = pos_tag(token1)\n",
        "print(taggedToken)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('And', 'CC'), ('yes', 'UH'), (',', ','), ('she', 'PRP'), ('does', 'VBZ'), ('mean', 'VB'), ('everybody', 'NN'), (\"'s\", 'POS'), ('job', 'NN'), ('from', 'IN'), ('yours', 'NNS'), ('to', 'TO'), ('mine', 'VB'), ('and', 'CC'), ('onward', 'VB'), ('to', 'TO'), ('the', 'DT'), ('role', 'NN'), ('of', 'IN'), ('grain', 'NN'), ('farmers', 'NNS'), ('in', 'IN'), ('Egypt', 'NNP'), (',', ','), ('pastry', 'NN'), ('chefs', 'NNS'), ('in', 'IN'), ('Paris', 'NNP'), ('and', 'CC'), ('dog', 'NN'), ('walkers', 'NNS'), ('in', 'IN'), ('Oregon', 'NNP'), ('i.e', 'NN'), ('.', '.'), ('every', 'DT'), ('job', 'NN'), ('.', '.'), ('We', 'PRP'), ('will', 'MD'), ('now', 'RB'), ('be', 'VB'), ('able', 'JJ'), ('to', 'TO'), ('help', 'VB'), ('direct', 'VB'), ('all', 'DT'), ('workers', 'NNS'), ('’', 'VBP'), ('actions', 'NNS'), ('and', 'CC'), ('behavior', 'NN'), ('with', 'IN'), ('a', 'DT'), ('new', 'JJ'), ('degree', 'NN'), ('of', 'IN'), ('intelligence', 'NN'), ('that', 'WDT'), ('comes', 'VBZ'), ('from', 'IN'), ('predictive', 'JJ'), ('analytics', 'NNS'), (',', ','), ('all', 'DT'), ('stemming', 'VBG'), ('from', 'IN'), ('the', 'DT'), ('AI', 'NNP'), ('engines', 'VBZ'), ('we', 'PRP'), ('will', 'MD'), ('now', 'RB'), ('increasingly', 'RB'), ('depend', 'VBP'), ('upon', 'NN'), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJ-7cxeFFdpd",
        "colab_type": "text"
      },
      "source": [
        "## 1.4 개체명 인식(NER, Named Entity Recognition)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCmFKvdLFkj6",
        "colab_type": "code",
        "outputId": "d8cbd9be-db61-4716-b4b2-b596520ac911",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "#개체가 뭔지 인식하기 위해 미리 존재하는 사전을 사용\n",
        "nltk.download('words')  \n",
        "nltk.download('maxent_ne_chunker')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dcA2f1AGyTr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#검색 엔진의 핵심기술\n",
        "from nltk import ne_chunk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nK0copDKHKCN",
        "colab_type": "code",
        "outputId": "c19f8c48-83e5-4e19-a78e-761b57a6430f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1343
        }
      },
      "source": [
        "neToken = ne_chunk(taggedToken)\n",
        "print(neToken)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  And/CC\n",
            "  yes/UH\n",
            "  ,/,\n",
            "  she/PRP\n",
            "  does/VBZ\n",
            "  mean/VB\n",
            "  everybody/NN\n",
            "  's/POS\n",
            "  job/NN\n",
            "  from/IN\n",
            "  yours/NNS\n",
            "  to/TO\n",
            "  mine/VB\n",
            "  and/CC\n",
            "  onward/VB\n",
            "  to/TO\n",
            "  the/DT\n",
            "  role/NN\n",
            "  of/IN\n",
            "  grain/NN\n",
            "  farmers/NNS\n",
            "  in/IN\n",
            "  (GPE Egypt/NNP)\n",
            "  ,/,\n",
            "  pastry/NN\n",
            "  chefs/NNS\n",
            "  in/IN\n",
            "  (GPE Paris/NNP)\n",
            "  and/CC\n",
            "  dog/NN\n",
            "  walkers/NNS\n",
            "  in/IN\n",
            "  (GPE Oregon/NNP)\n",
            "  i.e/NN\n",
            "  ./.\n",
            "  every/DT\n",
            "  job/NN\n",
            "  ./.\n",
            "  We/PRP\n",
            "  will/MD\n",
            "  now/RB\n",
            "  be/VB\n",
            "  able/JJ\n",
            "  to/TO\n",
            "  help/VB\n",
            "  direct/VB\n",
            "  all/DT\n",
            "  workers/NNS\n",
            "  ’/VBP\n",
            "  actions/NNS\n",
            "  and/CC\n",
            "  behavior/NN\n",
            "  with/IN\n",
            "  a/DT\n",
            "  new/JJ\n",
            "  degree/NN\n",
            "  of/IN\n",
            "  intelligence/NN\n",
            "  that/WDT\n",
            "  comes/VBZ\n",
            "  from/IN\n",
            "  predictive/JJ\n",
            "  analytics/NNS\n",
            "  ,/,\n",
            "  all/DT\n",
            "  stemming/VBG\n",
            "  from/IN\n",
            "  the/DT\n",
            "  (ORGANIZATION AI/NNP)\n",
            "  engines/VBZ\n",
            "  we/PRP\n",
            "  will/MD\n",
            "  now/RB\n",
            "  increasingly/RB\n",
            "  depend/VBP\n",
            "  upon/NN\n",
            "  ./.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFQpocASHvzI",
        "colab_type": "text"
      },
      "source": [
        "## 1.5 원형 추출"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zn-_78SUH14F",
        "colab_type": "text"
      },
      "source": [
        "1.5.1 어간추출(Stemming)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NtguVnZHNz7",
        "colab_type": "code",
        "outputId": "89df1c14-f72a-4842-bfc0-4d720de4ecb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "#품사가 훼손, 어휘 자체가 불완전해서 Stemming을 많이 사용하진 않음\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "print(\"running -> \" + ps.stem(\"running\"))\n",
        "print(\"beautiful -> \" + ps.stem(\"beautiful\"))\n",
        "print(\"believes -> \" + ps.stem(\"believes\"))\n",
        "print(\"using -> \" + ps.stem(\"using\"))\n",
        "print(\"conversation -> \" + ps.stem(\"conversation\"))\n",
        "print(\"organization -> \" + ps.stem(\"organization\"))\n",
        "print(\"studies -> \" + ps.stem(\"studies\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running -> run\n",
            "beautiful -> beauti\n",
            "believes -> believ\n",
            "using -> use\n",
            "conversation -> convers\n",
            "organization -> organ\n",
            "studies -> studi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hljm0UnIFo8",
        "colab_type": "code",
        "outputId": "959f55da-c134-470b-c664-29a4ffb55891",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ra7NFS2_I9BS",
        "colab_type": "text"
      },
      "source": [
        "1.5.2 표제어 추출(Lemmatization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BweHAGCZIoGd",
        "colab_type": "code",
        "outputId": "d353aa54-db55-4b7c-9bfc-b30d9699e0f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wl = WordNetLemmatizer()\n",
        "\n",
        "print(\"running -> \" + wl.lemmatize(\"running\"))\n",
        "print(\"beautiful -> \" + wl.lemmatize(\"beautiful\"))\n",
        "print(\"believes -> \" + wl.lemmatize(\"believes\"))\n",
        "print(\"using -> \" + wl.lemmatize(\"using\"))\n",
        "print(\"conversation -> \" + wl.lemmatize(\"conversation\"))\n",
        "print(\"organization -> \" + wl.lemmatize(\"organization\"))\n",
        "print(\"studies -> \" + wl.lemmatize(\"studies\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running -> running\n",
            "beautiful -> beautiful\n",
            "believes -> belief\n",
            "using -> using\n",
            "conversation -> conversation\n",
            "organization -> organization\n",
            "studies -> study\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dnnx_d5XJl-A",
        "colab_type": "text"
      },
      "source": [
        "## 1.6 불용어 처리(Stonword)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52NzdSsZKdhl",
        "colab_type": "code",
        "outputId": "8ca47e97-4d9c-4cc6-9dd3-31103b09952f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1020
        }
      },
      "source": [
        "#최빈어 조회, 최빈어를 조회하여 불용어 제거 대상을 선정\n",
        "from collections import Counter\n",
        "Counter(taggedToken).most_common()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[((',', ','), 3),\n",
              " (('from', 'IN'), 3),\n",
              " (('to', 'TO'), 3),\n",
              " (('and', 'CC'), 3),\n",
              " (('in', 'IN'), 3),\n",
              " (('.', '.'), 3),\n",
              " (('job', 'NN'), 2),\n",
              " (('the', 'DT'), 2),\n",
              " (('of', 'IN'), 2),\n",
              " (('will', 'MD'), 2),\n",
              " (('now', 'RB'), 2),\n",
              " (('all', 'DT'), 2),\n",
              " (('And', 'CC'), 1),\n",
              " (('yes', 'UH'), 1),\n",
              " (('she', 'PRP'), 1),\n",
              " (('does', 'VBZ'), 1),\n",
              " (('mean', 'VB'), 1),\n",
              " (('everybody', 'NN'), 1),\n",
              " ((\"'s\", 'POS'), 1),\n",
              " (('yours', 'NNS'), 1),\n",
              " (('mine', 'VB'), 1),\n",
              " (('onward', 'VB'), 1),\n",
              " (('role', 'NN'), 1),\n",
              " (('grain', 'NN'), 1),\n",
              " (('farmers', 'NNS'), 1),\n",
              " (('Egypt', 'NNP'), 1),\n",
              " (('pastry', 'NN'), 1),\n",
              " (('chefs', 'NNS'), 1),\n",
              " (('Paris', 'NNP'), 1),\n",
              " (('dog', 'NN'), 1),\n",
              " (('walkers', 'NNS'), 1),\n",
              " (('Oregon', 'NNP'), 1),\n",
              " (('i.e', 'NN'), 1),\n",
              " (('every', 'DT'), 1),\n",
              " (('We', 'PRP'), 1),\n",
              " (('be', 'VB'), 1),\n",
              " (('able', 'JJ'), 1),\n",
              " (('help', 'VB'), 1),\n",
              " (('direct', 'VB'), 1),\n",
              " (('workers', 'NNS'), 1),\n",
              " (('’', 'VBP'), 1),\n",
              " (('actions', 'NNS'), 1),\n",
              " (('behavior', 'NN'), 1),\n",
              " (('with', 'IN'), 1),\n",
              " (('a', 'DT'), 1),\n",
              " (('new', 'JJ'), 1),\n",
              " (('degree', 'NN'), 1),\n",
              " (('intelligence', 'NN'), 1),\n",
              " (('that', 'WDT'), 1),\n",
              " (('comes', 'VBZ'), 1),\n",
              " (('predictive', 'JJ'), 1),\n",
              " (('analytics', 'NNS'), 1),\n",
              " (('stemming', 'VBG'), 1),\n",
              " (('AI', 'NNP'), 1),\n",
              " (('engines', 'VBZ'), 1),\n",
              " (('we', 'PRP'), 1),\n",
              " (('increasingly', 'RB'), 1),\n",
              " (('depend', 'VBP'), 1),\n",
              " (('upon', 'NN'), 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "milctN-3KxQB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopPos = ['IN', 'CC', 'UH', 'TO', 'MD', 'DT', 'VBZ', 'VBP']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GWxIrNeLhEW",
        "colab_type": "code",
        "outputId": "47b6a729-aba5-47e6-a9b5-86449be9a53b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "stopWord = [',', 'be', 'able']\n",
        "\n",
        "word = []\n",
        "for tag in taggedToken:\n",
        "  if tag[1] not in stopPos:\n",
        "    if tag[0] not in stopWord:\n",
        "      word.append(tag[0])\n",
        "      \n",
        "print(word)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['she', 'mean', 'everybody', \"'s\", 'job', 'yours', 'mine', 'onward', 'role', 'grain', 'farmers', 'Egypt', 'pastry', 'chefs', 'Paris', 'dog', 'walkers', 'Oregon', 'i.e', '.', 'job', '.', 'We', 'now', 'help', 'direct', 'workers', 'actions', 'behavior', 'new', 'degree', 'intelligence', 'that', 'predictive', 'analytics', 'stemming', 'AI', 'we', 'now', 'increasingly', 'upon', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaaU0ZSHL3JU",
        "colab_type": "text"
      },
      "source": [
        "## 1.7 영문 텍스트 전처리 종합"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er3_ARhWLxiZ",
        "colab_type": "code",
        "outputId": "51108ec8-5b65-46d0-d2f9-d5a3de09aada",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('words')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "from nltk import pos_tag\n",
        "from nltk.ste import\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxRo5vQbMDCl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}